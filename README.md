# Data_Pipeline-for-Processing-CSV

![te](https://github.com/YomiDavies/Data_Pipeline-for-Processing-CSV/blob/main/Architecture%20for%20DataPipeline%20processing.jpeg)

ğŸš€ Project Highlight: Building a Serverless Data Pipeline on AWS

Iâ€™m excited to share a recent project where I designed and deployed a serverless data pipeline using AWS to automate the ingestion, transformation, and visualization of CSV data.

Hereâ€™s how the pipeline works:

ğŸ“¥ Data Ingestion: CSV files are uploaded to an S3 bucket, acting as the central storage for both raw and processed data.
âš™ï¸ Trigger & Transformation: An AWS Lambda function is automatically triggered to clean and reformat the data, which is then passed to AWS Glue for deeper ETL operations.
ğŸ—‚ï¸ Data Storage: The processed data is stored in a dedicated S3 bucket.
ğŸ“Š Visualization: Amazon QuickSight connects to the final dataset and generates interactive dashboards and reports.

ğŸ› ï¸ Services Used:

Amazon S3 â€“ Storage for raw and processed data

AWS Lambda â€“ Serverless processing for automation

AWS Glue â€“ ETL operations for structured transformation

Amazon QuickSight â€“ Dashboards and reporting

IAM Roles & Policies â€“ Securing inter-service access

This project helped sharpen my skills in cloud automation, serverless computing, and real-time data visualizationâ€”all in a production-style environment. Itâ€™s a great example of building modern solutions without managing infrastructure.
